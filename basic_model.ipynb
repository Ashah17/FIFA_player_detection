{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.2.103 ðŸš€ Python-3.9.12 torch-2.0.0 CPU (Apple M2)\n",
      "Setup complete âœ… (8 CPUs, 16.0 GB RAM, 449.1/460.4 GB disk)\n"
     ]
    }
   ],
   "source": [
    "!pip install ultralytics==8.2.103 \"opencv-python-headless<4.9\" easyocr scikit-learn pandas numpy -q\n",
    "\n",
    "from IPython import display\n",
    "display.clear_output()\n",
    "\n",
    "# Import libraries\n",
    "import ultralytics\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import shutil\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from sklearn.cluster import KMeans\n",
    "import easyocr\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Prevent ultralytics from tracking\n",
    "!yolo settings sync=False\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generic Helper Functions ---\n",
    "def load_dataset_split(base_dir, split_name):\n",
    "    \"\"\"Loads a dataset split into a dataframe.\"\"\"\n",
    "    split_dir = Path(base_dir) / split_name\n",
    "    images_dir = split_dir / 'images'\n",
    "    labels_dir = split_dir / 'labels'\n",
    "\n",
    "    # --- NEW CHECK ---\n",
    "    if not images_dir.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Error: The directory '{images_dir}' does not exist. \"\n",
    "            \"Please check that your dataset downloaded and unzipped correctly. \"\n",
    "            f\"Expected structure: {base_dir}/{split_name}/images\"\n",
    "        )\n",
    "    # --- END NEW CHECK ---\n",
    "    \n",
    "    image_files = sorted(images_dir.glob('*.jpg'))\n",
    "\n",
    "    # --- NEW CHECK ---\n",
    "    if not image_files:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Error: No '.jpg' files were found in '{images_dir}'. \"\n",
    "            \"The directory is empty or the download may have failed.\"\n",
    "        )\n",
    "    # --- END NEW CHECK ---\n",
    "    \n",
    "    data = []\n",
    "    for img_path in image_files:\n",
    "        label_filename = img_path.stem + '.txt'\n",
    "        label_path = labels_dir / label_filename\n",
    "        data.append({\n",
    "            'image_path': str(img_path),\n",
    "            'label_path': str(label_path) if label_path.exists() else None,\n",
    "            'filename': img_path.name,\n",
    "            'split': split_name\n",
    "        })\n",
    "    \n",
    "    print(f\"  Loaded {len(data)} images from {images_dir}\") # Added for visibility\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# --- Rotation/Augmentation Functions ---\n",
    "\n",
    "def rotate_bbox_90(x_center, y_center, width, height, angle):\n",
    "    \"\"\"Rotate bounding box coordinates for 90, 180, or 270 degree rotations.\"\"\"\n",
    "    if angle == 90:\n",
    "        return 1 - y_center, x_center, height, width\n",
    "    elif angle == 180:\n",
    "        return 1 - x_center, 1 - y_center, width, height\n",
    "    elif angle == 270:\n",
    "        return y_center, 1 - x_center, height, width\n",
    "    raise ValueError(\"Angle must be 90, 180, or 270\")\n",
    "\n",
    "def rotate_keypoint_90(x, y, angle):\n",
    "    \"\"\"Rotate keypoint coordinates for 90, 180, or 270 degree rotations.\"\"\"\n",
    "    if angle == 90:\n",
    "        return 1 - y, x\n",
    "    elif angle == 180:\n",
    "        return 1 - x, 1 - y\n",
    "    elif angle == 270:\n",
    "        return y, 1 - x\n",
    "    raise ValueError(\"Angle must be 90, 180, or 270\")\n",
    "\n",
    "def rotate_image_90(image_path, angle):\n",
    "    \"\"\"Rotate an image by 90, 180, or 270 degrees.\"\"\"\n",
    "    img = Image.open(image_path)\n",
    "    if angle == 90:\n",
    "        return img.rotate(-90, expand=True)\n",
    "    elif angle == 180:\n",
    "        return img.rotate(-180, expand=True)\n",
    "    elif angle == 270:\n",
    "        return img.rotate(-270, expand=True)\n",
    "    raise ValueError(\"Angle must be 90, 180, or 270\")\n",
    "\n",
    "# --- Field Dataset (Keypoints) Functions ---\n",
    "FIELD_CLASS_NAMES = ['pitch']\n",
    "NUM_KEYPOINTS = 32\n",
    "\n",
    "def parse_field_label(label_path):\n",
    "    \"\"\"Parse a YOLO format label file with keypoints.\"\"\"\n",
    "    if label_path is None or not Path(label_path).exists():\n",
    "        return []\n",
    "    annotations = []\n",
    "    with open(label_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 5:\n",
    "                class_id, x_c, y_c, w, h = int(parts[0]), float(parts[1]), float(parts[2]), float(parts[3]), float(parts[4])\n",
    "                keypoints = []\n",
    "                num_visible_keypoints = 0\n",
    "                for i in range(5, len(parts), 3):\n",
    "                    if i + 2 < len(parts):\n",
    "                        kpt_x, kpt_y, kpt_vis = float(parts[i]), float(parts[i+1]), int(float(parts[i+2]))\n",
    "                        keypoints.append((kpt_x, kpt_y, kpt_vis))\n",
    "                        if kpt_vis > 0: num_visible_keypoints += 1\n",
    "                annotations.append({\n",
    "                    'class_id': class_id, 'class_name': FIELD_CLASS_NAMES[class_id],\n",
    "                    'x_center': x_c, 'y_center': y_c, 'width': w, 'height': h,\n",
    "                    'keypoints': keypoints, 'num_visible_keypoints': num_visible_keypoints\n",
    "                })\n",
    "    return annotations\n",
    "\n",
    "def add_field_label_info(df):\n",
    "    \"\"\"Add field label info to the dataframe.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['annotations'] = df['label_path'].apply(parse_field_label)\n",
    "    df['num_pitches'] = df['annotations'].apply(len)\n",
    "    df['avg_visible_keypoints'] = df['annotations'].apply(\n",
    "        lambda anns: np.mean([ann['num_visible_keypoints'] for ann in anns]) if anns else 0\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def augment_field_dataset(df, base_dir, split_name, prob=0.5, angles=[90, 180, 270]):\n",
    "    \"\"\"Apply rotation augmentation to the field dataset.\"\"\"\n",
    "    aug_images_dir = Path(base_dir) / split_name / 'images_augmented'\n",
    "    aug_labels_dir = Path(base_dir) / split_name / 'labels_augmented'\n",
    "    aug_images_dir.mkdir(exist_ok=True); aug_labels_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(f\"Augmenting field {split_name} set...\")\n",
    "    aug_count = 0\n",
    "    for idx, row in df.iterrows():\n",
    "        if random.random() < prob:\n",
    "            angle = random.choice(angles)\n",
    "            rotated_img = rotate_image_90(row['image_path'], angle)\n",
    "            \n",
    "            base_filename = Path(row['filename']).stem\n",
    "            new_filename = f\"{base_filename}_rot{angle}.jpg\"\n",
    "            new_img_path = aug_images_dir / new_filename\n",
    "            new_label_path = aug_labels_dir / f\"{base_filename}_rot{angle}.txt\"\n",
    "            rotated_img.save(new_img_path)\n",
    "            \n",
    "            with open(new_label_path, 'w') as f:\n",
    "                for ann in row['annotations']:\n",
    "                    new_x, new_y, new_w, new_h = rotate_bbox_90(\n",
    "                        ann['x_center'], ann['y_center'], ann['width'], ann['height'], angle\n",
    "                    )\n",
    "                    \n",
    "                    label_line = f\"{ann['class_id']} {new_x} {new_y} {new_w} {new_h}\"\n",
    "                    for kpt_x, kpt_y, kpt_vis in ann['keypoints']:\n",
    "                        new_kpt_x, new_kpt_y = (rotate_keypoint_90(kpt_x, kpt_y, angle) if kpt_vis > 0 else (kpt_x, kpt_y))\n",
    "                        label_line += f\" {new_kpt_x} {new_kpt_y} {kpt_vis}\"\n",
    "                    f.write(label_line + \"\\n\")\n",
    "            aug_count += 1\n",
    "    print(f\"  Created {aug_count} augmented field samples for {split_name}.\")\n",
    "\n",
    "# --- Player Dataset (Detection) Functions ---\n",
    "PLAYER_CLASS_NAMES = ['ball', 'goalkeeper', 'player', 'referee']\n",
    "\n",
    "def parse_player_label(label_path):\n",
    "    \"\"\"Parse a YOLO format label file (detection).\"\"\"\n",
    "    if label_path is None or not Path(label_path).exists():\n",
    "        return []\n",
    "    annotations = []\n",
    "    with open(label_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 5:\n",
    "                class_id, x_c, y_c, w, h = int(parts[0]), float(parts[1]), float(parts[2]), float(parts[3]), float(parts[4])\n",
    "                annotations.append({\n",
    "                    'class_id': class_id, 'class_name': PLAYER_CLASS_NAMES[class_id],\n",
    "                    'x_center': x_c, 'y_center': y_c, 'width': w, 'height': h\n",
    "                })\n",
    "    return annotations\n",
    "\n",
    "def add_player_label_info(df):\n",
    "    \"\"\"Add player label info to the dataframe.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['annotations'] = df['label_path'].apply(parse_player_label)\n",
    "    df['num_objects'] = df['annotations'].apply(len)\n",
    "    for class_id, class_name in enumerate(PLAYER_CLASS_NAMES):\n",
    "        df[f'num_{class_name}s'] = df['annotations'].apply(\n",
    "            lambda anns: sum(1 for ann in anns if ann['class_id'] == class_id)\n",
    "        )\n",
    "    return df\n",
    "\n",
    "def augment_player_dataset(df, base_dir, split_name, prob=0.5, angles=[90, 180, 270]):\n",
    "    \"\"\"Apply rotation augmentation to the player dataset.\"\"\"\n",
    "    aug_images_dir = Path(base_dir) / split_name / 'images_augmented'\n",
    "    aug_labels_dir = Path(base_dir) / split_name / 'labels_augmented'\n",
    "    aug_images_dir.mkdir(exist_ok=True); aug_labels_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(f\"Augmenting player {split_name} set...\")\n",
    "    aug_count = 0\n",
    "    for idx, row in df.iterrows():\n",
    "        if random.random() < prob:\n",
    "            angle = random.choice(angles)\n",
    "            rotated_img = rotate_image_90(row['image_path'], angle)\n",
    "            \n",
    "            base_filename = Path(row['filename']).stem\n",
    "            new_filename = f\"{base_filename}_rot{angle}.jpg\"\n",
    "            new_img_path = aug_images_dir / new_filename\n",
    "            new_label_path = aug_labels_dir / f\"{base_filename}_rot{angle}.txt\"\n",
    "            rotated_img.save(new_img_path)\n",
    "            \n",
    "            with open(new_label_path, 'w') as f:\n",
    "                for ann in row['annotations']:\n",
    "                    new_x, new_y, new_w, new_h = rotate_bbox_90(\n",
    "                        ann['x_center'], ann['y_center'], ann['width'], ann['height'], angle\n",
    "                    )\n",
    "                    f.write(f\"{ann['class_id']} {new_x} {new_y} {new_w} {new_h}\\n\")\n",
    "            aug_count += 1\n",
    "    print(f\"  Created {aug_count} augmented player samples for {split_name}.\")\n",
    "\n",
    "# --- YAML Creation Function ---\n",
    "def create_dataset_yaml(data_dir, class_names, kpt_shape=None):\n",
    "    \"\"\"Creates the data.yaml file for YOLO training.\"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    \n",
    "    # Create main directories if they don't exist\n",
    "    (data_dir / 'train').mkdir(exist_ok=True)\n",
    "    (data_dir / 'valid').mkdir(exist_ok=True)\n",
    "    (data_dir / 'test').mkdir(exist_ok=True)\n",
    "    \n",
    "    # Move original images/labels\n",
    "    shutil.move(str(data_dir / 'train' / 'images'), str(data_dir / 'train' / 'images_orig'))\n",
    "    shutil.move(str(data_dir / 'train' / 'labels'), str(data_dir / 'train' / 'labels_orig'))\n",
    "    shutil.move(str(data_dir / 'valid' / 'images'), str(data_dir / 'valid' / 'images_orig'))\n",
    "    shutil.move(str(data_dir / 'valid' / 'labels'), str(data_dir / 'valid' / 'labels_orig'))\n",
    "    shutil.move(str(data_dir / 'test' / 'images'), str(data_dir / 'test' / 'images_orig'))\n",
    "    shutil.move(str(data_dir / 'test' / 'labels'), str(data_dir / 'test' / 'labels_orig'))\n",
    "    \n",
    "    # Rename augmented dirs to be the new train/valid/test dirs\n",
    "    shutil.move(str(data_dir / 'train' / 'images_augmented'), str(data_dir / 'train' / 'images'))\n",
    "    shutil.move(str(data_dir / 'train' / 'labels_augmented'), str(data_dir / 'train' / 'labels'))\n",
    "    shutil.move(str(data_dir / 'valid' / 'images_augmented'), str(data_dir / 'valid' / 'images'))\n",
    "    shutil.move(str(data_dir / 'valid' / 'labels_augmented'), str(data_dir / 'valid' / 'labels'))\n",
    "    shutil.move(str(data_dir / 'test' / 'images_augmented'), str(data_dir / 'test' / 'images'))\n",
    "    shutil.move(str(data_dir / 'test' / 'labels_augmented'), str(data_dir / 'test' / 'labels'))\n",
    "    \n",
    "    # Copy original files into the new augmented directories\n",
    "    for split in ['train', 'valid', 'test']:\n",
    "        for f in (data_dir / split / 'images_orig').glob('*.jpg'):\n",
    "            shutil.copy(f, data_dir / split / 'images')\n",
    "        for f in (data_dir / split / 'labels_orig').glob('*.txt'):\n",
    "            shutil.copy(f, data_dir / split / 'labels')\n",
    "    \n",
    "    # Create data.yaml\n",
    "\n",
    "    train_abs_path = (data_dir / 'train' / 'images').absolute()\n",
    "    val_abs_path = (data_dir / 'valid' / 'images').absolute()\n",
    "    test_abs_path = (data_dir / 'test' / 'images').absolute()\n",
    "    \n",
    "    # Create data.yaml\n",
    "    yaml_content = f\"\"\"\n",
    "train: {train_abs_path}\n",
    "val: {val_abs_path}\n",
    "test: {test_abs_path}\n",
    "\n",
    "names: {class_names}\n",
    "nc: {len(class_names)}\n",
    "\"\"\"\n",
    "    \n",
    "    if kpt_shape:\n",
    "        yaml_content += f\"\\nkpt_shape: {kpt_shape}\\n\"\n",
    "        \n",
    "    yaml_path = data_dir / 'data.yaml'\n",
    "    with open(yaml_path, 'w') as f:\n",
    "        f.write(yaml_content)\n",
    "        \n",
    "    print(f\"Created {yaml_path} with ABSOLUTE paths.\")\n",
    "    return str(yaml_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded 298 images from player_dataset/train/images\n",
      "  Loaded 49 images from player_dataset/valid/images\n",
      "  Loaded 25 images from player_dataset/test/images\n",
      "Augmenting player train set...\n",
      "  Created 148 augmented player samples for train.\n",
      "Augmenting player valid set...\n",
      "  Created 10 augmented player samples for valid.\n",
      "Augmenting player test set...\n",
      "  Created 12 augmented player samples for test.\n",
      "Created player_dataset/data.yaml with ABSOLUTE paths.\n",
      "  Loaded 255 images from field_dataset/train/images\n",
      "  Loaded 34 images from field_dataset/valid/images\n",
      "  Loaded 28 images from field_dataset/test/images\n",
      "Augmenting field train set...\n",
      "  Created 129 augmented field samples for train.\n",
      "Augmenting field valid set...\n",
      "  Created 12 augmented field samples for valid.\n",
      "Augmenting field test set...\n",
      "  Created 9 augmented field samples for test.\n",
      "Created field_dataset/data.yaml with ABSOLUTE paths.\n"
     ]
    }
   ],
   "source": [
    "# --- Process Player Dataset ---\n",
    "player_base_dir = 'player_dataset'\n",
    "player_train_df = load_dataset_split(player_base_dir, 'train')\n",
    "player_valid_df = load_dataset_split(player_base_dir, 'valid')\n",
    "player_test_df = load_dataset_split(player_base_dir, 'test')\n",
    "\n",
    "player_train_df = add_player_label_info(player_train_df)\n",
    "player_valid_df = add_player_label_info(player_valid_df)\n",
    "player_test_df = add_player_label_info(player_test_df)\n",
    "\n",
    "augment_player_dataset(player_train_df, player_base_dir, 'train', prob=0.5)\n",
    "augment_player_dataset(player_valid_df, player_base_dir, 'valid', prob=0.3)\n",
    "augment_player_dataset(player_test_df, player_base_dir, 'test', prob=0.3)\n",
    "\n",
    "player_yaml_path = create_dataset_yaml(player_base_dir, PLAYER_CLASS_NAMES)\n",
    "\n",
    "# --- Process Field Dataset ---\n",
    "field_base_dir = 'field_dataset'\n",
    "field_train_df = load_dataset_split(field_base_dir, 'train')\n",
    "field_valid_df = load_dataset_split(field_base_dir, 'valid')\n",
    "field_test_df = load_dataset_split(field_base_dir, 'test')\n",
    "\n",
    "field_train_df = add_field_label_info(field_train_df)\n",
    "field_valid_df = add_field_label_info(field_valid_df)\n",
    "field_test_df = add_field_label_info(field_test_df)\n",
    "\n",
    "augment_field_dataset(field_train_df, field_base_dir, 'train', prob=0.5)\n",
    "augment_field_dataset(field_valid_df, field_base_dir, 'valid', prob=0.3)\n",
    "augment_field_dataset(field_test_df, field_base_dir, 'test', prob=0.3)\n",
    "\n",
    "field_yaml_path = create_dataset_yaml(field_base_dir, FIELD_CLASS_NAMES, kpt_shape=[NUM_KEYPOINTS, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LIGHTWEIGHT Settings: Batch=4, Workers=0, Image Size=320\n",
      "\n",
      "--- Starting Player Detector Training (Lightweight Mode) ---\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8n.pt to 'yolov8n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.25M/6.25M [00:00<00:00, 7.66MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.224 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.2.103 ðŸš€ Python-3.9.12 torch-2.0.0 CPU (Apple M2)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=player_dataset/data.yaml, epochs=10, time=None, patience=100, batch=4, imgsz=320, save=True, save_period=-1, cache=False, device=None, workers=0, project=Initial_Evaluation, name=player_detection_lightweight, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=Initial_Evaluation/player_detection_lightweight\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "import ultralytics\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# --- Configuration for Maximum Stability ---\n",
    "# Lowest resource footprint settings for CPU training\n",
    "STABLE_BATCH_SIZE = 4 \n",
    "STABLE_WORKERS = 0  \n",
    "LIGHTWEIGHT_IMG_SIZE = 320 # <--- NEW: Use 320x320 pixels instead of 640x640\n",
    "\n",
    "print(f\"Using LIGHTWEIGHT Settings: Batch={STABLE_BATCH_SIZE}, Workers={STABLE_WORKERS}, Image Size={LIGHTWEIGHT_IMG_SIZE}\")\n",
    "\n",
    "print(\"\\n--- Starting Player Detector Training (Lightweight Mode) ---\")\n",
    "\n",
    "player_model = YOLO('yolov8n.pt') \n",
    "\n",
    "player_detector_results = player_model.train(\n",
    "    data=player_yaml_path,\n",
    "    epochs=10,            \n",
    "    imgsz=LIGHTWEIGHT_IMG_SIZE, # <--- CRITICAL CHANGE\n",
    "    project='Initial_Evaluation',\n",
    "    name='player_detection_lightweight',\n",
    "    batch=STABLE_BATCH_SIZE, \n",
    "    workers=STABLE_WORKERS   \n",
    ")\n",
    "print(\"Player Detector Training Complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cbdf1acd52ec173fdc3f72ba5b15265fa16f9b4912ff0cbbea2897f3c4f7fcd3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
